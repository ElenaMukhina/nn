{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Deep Learning\n",
    "\n",
    "Потенциал ИИ в подражении человеческим мыслительным процессам выходит за рамки пассивных задач, таких как распознования объектов, или реактивных задач, таких как вождение автомобиля. \n",
    "\n",
    "Сейчас он распространяется на всевозможную творческую деятельность.\n",
    "\n",
    "* 2014 год - первые экспементы;\n",
    "* 2015 год - Google Deep Dream;\n",
    "* 2016 год - Prisma application;\n",
    "* Лето 2016 года - экспериментальный фильм Sunspring - сценарий написан LSTM - полный диалоги!\n",
    "* Конец 2016 года - музыка, написанная нейросетью;\n",
    "\n",
    "Разумеется, пока художественные и творческие постановки от AI не совсем качественны. AI нисколько не близок к соперничеству со сценаристами, художниками и композиторами. \n",
    "\n",
    "Во многих областях (но особенно творческих), AI будеи использоваться людьми как инструмент для увеличения своих возможностей.\n",
    "\n",
    "Большая часть художественного творчества состоит из простого распознавания образов и технического мастерства. И это именно та часть процесса, которую многие находят наимени привлекательной и даже пытаются её пропустить. И это то, где AI может помочь нам. Наши перцептивные модальности, наш язык, наши творческие работы все это имеет статистическую структуру. \n",
    "\n",
    "Изучение этой структуры - именно то, где могут помочь алгоритмы глубокого обучения. Модели машинного обучения могут изучить статистическое \"латентной пространство\" изображений, или музыки, или даже историй и затем просто напросто сэмплировать из этого пространства, порождая новые работы с похожими характеристиками, какие встречались в тренировочных данных. Естественно, такая выборка вряд ли является актом художественного творчества сама по себе. Это простая математическая операция: алгоритм не имеет представления о человеческой жизни, человеческих эмоциях, нашем опыте мира: вместо этого он учится на «опыте», который имеет мало общего с нашим. Только наша интерпретация, как людей, как зрителей, даст смысл тому, что создала модель.\n",
    "\n",
    "В руках художника алгоритмическая генерация может стать более осмысленной и красивой. Сэмплирование из латентного пространства (Latent space sampling) может стать кистью, которая снабжает художника новым инструментом, дополняет творческие преимущества, расширяет пространство того, что мы можем себе представить.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Повторение: генерация текста с помощью LSTM\n",
    "\n",
    "Рассмотрим под несколько другим углом на уже рассматриваемый пример генерации текста. Текст по своей сути является последовательностью, и пример с генерацией текста может быть обобщен на любые последовательные данные! Вы можете применить их, например, к последовательности нот, для генерации музыки.\n",
    "\n",
    "Последовательная генерация данных не ограничивается созданием художественного контента. Она успешно применяется в:\n",
    "* синтезе речи;\n",
    "* генерации диалогов для чатботов;\n",
    "\n",
    "Технология \"Smart reply\" от Googla - способна автоматически генерировать выбор быстрых ответов на ваши сообщения, использует похожие техники.\n",
    "\n",
    "### Краткая история генеративных нейронных сетей\n",
    "\n",
    "В конце 2014, мало кто слышал аббревиатуру LSTM, даже в среде практиков машинного обучения. Однако, успешное применение генерации последовательных данных с помощью рекуррентных нейронных сетей стало повсеместно применяться в 2016. Эти технгики имеют долгую историю, которая началась с разработки алгоритма LSTM в 1997 году. этот алгоритм был использован для посимвольной генерации текста.\n",
    "\n",
    "В 2002 году Douglas Eck применил LSTM к генерации музыки с многообещающими результатами.\n",
    "\n",
    "В конце 2000х и ранних 2010-х Alex Graves сделал важную новаторскую работу об использовании рекуррентных нейронных сетей для генерации последовательных данных. \n",
    "\n",
    "Его работа в 2013 году по применению Recurrent Mixture Density Networks для генерации человекоподобного подчерка рассматривается некоторыми как поворотных момент. \n",
    "\n",
    "С тех пор RNN успешно применялись для генерации музыки, диалогов, генерации изображений, синтеза речи, разработки молекул, и даже создания фильмов.\n",
    "\n",
    "### Как мы можем генерировать последовательности данных?\n",
    "\n",
    "Универсальным способом генерировать последовательность данных в deep learning - обучить нейронную сеть (как правило, RNN или ConvNet) предсказать следующий токен или несколько токенов в последовательности с использованием предыдущих токенов в качестве входа.\n",
    "\n",
    "Например, по входу: *the cat is on the ma*, сеть обучается предсказывать следующий целевой символ *t*. Обычно при работе с текстовыми данными токенами выступают слова или символы, и любая подобная сеть может моделировать вероятность следующего токена по предыдущим, что называется **языковой моделью**. Языковая модель захватывает **латентное пространство (latent space)** языка, т.е. его статистическую структуру.\n",
    "\n",
    "Как только у нас будет такая обучаемая языковая модель, мы можем попробовать её в действии то есть сгенерировать новые последовательности - т.е. \"скормить\" некоторый текст (conditioning data), предсказать сл. символ или слово (мы можем также сгенерировать несколько слов за раз), а затем передать сгенерированные данные обратно на вход, повторив этот процесс неоднократно.\n",
    "\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/text_generation_process.png)\n",
    "\n",
    "Этот цикл позволяет нам сгенерировать последовательности произвольной длины, отражающие структуру данных, на которых модель обучена, то есть последовательности которые выглядят почти как предложения, написанные человеком. В нашем случае, мы берем LSTM модель, даем ей строку с N символами, выведенными из тестового корпуса, и обучаем предсказывать N+1 символ. Выходом модели будет SoftMax над всевозможными символами: вероятностное распределение для сл. символа. Эта LSTM называется character-level neural language model.\n",
    "\n",
    "### Важность выбора стратегии сэмплирования \n",
    "\n",
    "При генерации текста, способ, которым мы выбираем сл. символ очень важен. Наивный подход обычно называется **greedy sampling**, который состоит в выборе наиболее вероятного сл. символа. Однако, такой подход будет приводить к повторяющимся и слишком предсказуемым строкам, которые не выглядят как согласованный язык. Более интересный подход состоит в \"неожиданном выборе\": т.е. введением случайности в процесс сэмплирования, например, сэмплирование из вероятностного распределения для сл. символа. Это называется \"стохастическим сэмплированием\". В этом случае, если 'e' имеет вероятность 0.3 стать сл. символом в соответствии с модель, мы будем выбирать 'e' в 30% случаев. Заметим, что greedy sampling сам по себе выводитя из вероятностного распределения: определенный символ имеет вероятность 1, остальные нулевую вероятность.\n",
    "\n",
    "Вероятностное сэмплирование из выхода SoftMax является достаточно аккуратным, поскольку позволяет появляться даже маловероятным символам время от времени, таким образом, порождая интересные предложения и даже иногда демонстрируя творчество: новые, реалистично звучащие слова, которые не встречались в данных обучения! \n",
    "\n",
    "Но есть одна проблема с этой стратегией: она не предлагает способ контролировать случайность в процессе сэмплирования.\n",
    "\n",
    "Почему мы хотим контролировать случайность?\n",
    "Рассмотрим две экстремы: чисто случайное сэмплирование, т.е. порождение сл. символа из равномерного распределения, где каждый символ равновероятен. Эта схема будет иметь максимальную случайность, т.е. вероятностное распределение будет иметь максимальную энтропию. Естественно, это не дает ничего интересного.\n",
    "\n",
    "Другая экстрема: жадный поход, который также не порождает ничего интересного, не имеет никакой случайности: соответствующее вероятностное распределение имеем минимальную энтропию. Сэмплирование из реального вероятностного распределения, т.е. распределение которое выводится функцией SoftMax модели представляет собой промежуточную точку между этими двумя экстремами. Однако, есть много других промежуточных точек между максимальной и минимальной энтропией, которые мы бы хотели использовать. Меньшая нтропия даст генерируемые предложения более  предсказательной структуры (и поэтому, они потенциально будут выглядеть более реалистично), в то время как больше энтропии приведет к большим сюрпризам и достаточно творческим последовательностям. При сэмплировании из генеративной модели всегда хорошо исследовать разные степени случайности в процессе генерации. Но следует помнить, что конечными судьями являемся мы, люди, и наша оценка субъективна, и заранее нельзя дать совет, где лежит оптимальная энтропия.\n",
    "\n",
    "Чтобы контролировать стохастичность процесса сэмплирования, введем параметр называемый \"softmax temperature\", который характеризует энтропию вероятностного распределения используемую для сэмплирования, или другими словами, характеризует насколько \"необычным\" или предсказуемым будет наш выбор сл. символа. По заданному значению температуры, новое вероятностное распределение вычисляется на базе оригинального (softmax выхода модели) посредством \"взвешивания\" (reweighting) сл. образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    \"\"\"Reweight a probability distribution to increase or decrease entropy.\n",
    "\n",
    "    # Arguments\n",
    "        original_distribution: A 1D Numpy array of probability values.\n",
    "            Must sum to one.\n",
    "        temperature: Factor quantifying the entropy of the output distribution.\n",
    "\n",
    "    # Returns\n",
    "        A re-weighted version of the original distribution.\n",
    "    \"\"\"\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    # The sum of the distribution may no longer be 1!\n",
    "    # Thus we divide it by its sum to obtain the new distribution.\n",
    "    return distribution / np.sum(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более высокие значения \"температуры\" приводят к распределению выборки более высокой энтропии, что приведет к созданию более неожиданных и неструктурируемых сгенерированных данных, более низкие \"температуры\" приведут к меньшей случайности и более предсказуемым генерируемым данным.\n",
    "\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/temperature.png)\n",
    "\n",
    "### Реализация charcter-level LSTM generation\n",
    "\n",
    "Реализуем идеи на практике.\n",
    "\n",
    "Сначала нам нужно загрузить текст для обучения языковой модели. Это должен быть достаточно большой текстовый файл или целое множество. В нашем примере рассмотрим тексты Ницше. Языковая модель будет специфической моделью стиля Ницше и со специфичными темами (таким образом, лишь частная языковая модель англ. языка).\n",
    "\n",
    "**Подготовка данных**\n",
    "\n",
    "Загрузка корпуса и конвертация его в нижний регистр:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "581632/600901 [============================>.] - ETA: 0sCorpus length: 600901\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем извлекать частично-перекрывающиеся последовательности длины \n",
    "    maxlen\n",
    "унитарно кодируя их (one-hot encode) и затем упаковывая их в 3D NumPy массив x размерности \n",
    "    (sequences, maxlen, unique_characters).\n",
    "    \n",
    "Одновременно, мы подготавливаем массив y содержащий соответствующие целевые символы: унитарно закодированные символы, которые появляютсыя после показанной модели последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200281\n",
      "Unique characters: 59\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Построение сети**\n",
    "\n",
    "Построим один слой LSTM, за которым идет полносвязный слой Dense с функцией активации SoftMax. Но также отметим что RNN не единственный способ, которым можно провести генерации последовательностей данных; 1D свертка также показывает успешное решение этой задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используется унитарное кодирование для нашей целевой переменной, поэтому используем категориальную кросс-энтропию в качестве функции потерь для модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение языковой модели и сэмплирование из нее**\n",
    "\n",
    "Пусть дана обученная модель и задан некоторый текстовый снипет, мы генерируем новый текст повторяя сл. последовательность операций:\n",
    "\n",
    "1. Вывод из модели вероятностного распределения над сл. символом при уже уже доступном тексте;\n",
    "* Взвешивание (reweighting) распределения до определенной \"температуры\";\n",
    "* Сэмплирование сл. символа случайным образом в соответствии с измененным распределением;\n",
    "* Добавление нового символа в конец доступного текста.\n",
    "\n",
    "В коде мы используем \"повторное взвешивание\" (reweight) исходного вероятностного распределения, выведенного из модели, и генерируем символ из нее (т.е. функция сэмплинга)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, в цикл, где мы неоднократно обучаем и генерируем текстю Мы начинаем генерпацию текста с использованием диапазона различных температур после каждой эпохи. Это позволяет нам увидеть, как сгенерированный текст развивается по мере сходимости модели, а также влияние температуры в стратегии сэмплирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      " 39680/200281 [====>.........................] - ETA: 736s - loss: 2.4724"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b9fbc9abfb7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     model.fit(x, y,\n\u001b[0;32m      8\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m               epochs=1)\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Select a text seed at random\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\oleg-\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\oleg-\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\oleg-\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\oleg-\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\oleg-\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\oleg-\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\oleg-\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\oleg-\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\oleg-\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 60):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот что получается в 20 эпоху, задолго до сходимости модели. Мы использовали изначальный текст:\n",
    "\n",
    "С температурой 0.20:\n",
    "\n",
    "С температурой 0.50:\n",
    "\n",
    "С температурой 1.0:\n",
    "\n",
    "На двадцатой эпохе модель начинает сходиться и текст начинает выглядеть более когерентно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно увидеть, низкие температуры приводят к чрезвычайно повторяющемуся и предсказуемуму тексту, но где локальная структура чрезвычайно реалистична: в частности, все слова (слово является локальным паттерном символов) действительно являются реальными английскими словами.\n",
    "\n",
    "С ростом температуры, генерируемый текст становится более интересным, и даже креативным; иногда можно увидеть изобретение новых слов, которые тем не менее, звучат вполне согласовано (eterned, troveration). Ну а с высокими температурами, локальная структура начинает прерываться и большинство слов выглядят как по большей части случайные строки символов. \n",
    "\n",
    "Очевидно, 0.5 достаточно адекватное значение температуры для генерации текста при данных параметрах. **Всегда экспериментируйте с несколькими стратегиями сэмплинга!**\n",
    "Определенный баланс между обученной структурой и слуайностью делает генерацию более интересной.\n",
    "\n",
    "Заметим, что обучение большой модели является достаточно долгим и требует больше данных, но на ней вы можете достичь генерации текстов, которые будут выглядетьб более когерентно и реалистично.\n",
    "\n",
    "Но разумеется, не надо пока ожидать какого либо семантического значимго сгенерируемого текста: все что происходит - это выборка данных из статистической модели, в которой символы появляются после символов. Язык - это канал коммуникации и в этом различие между сообщениями и статистической структурой этих сообщений (статистической структурой в которых она кодируется). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюме**\n",
    "\n",
    "* Мы можем генерировать дискретные последовательности данных обучив модель предсказывать сл. токен по предыдущим токенам;\n",
    "* В случае текста, подобная модель называется языковой моделью (language model) и может базироваться на словах или символах;\n",
    "* Сэмплирование сл. токена требует баланса в мере случайности;\n",
    "* Одним из способов \"обращения\" со случайностью является нотация softmax temperature. Всегда экспериментируйте с различными температурами, чтобы найти ту, которая дает хорошие результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dream\n",
    "\n",
    "*Deep Dream* - представляет собой метод модификации художественного изображения, который использует представления, полученные сверточными сетями.\n",
    "\n",
    "Техника была представлена компанией Google в 2015 году, в качестве реализации написанной с использованием библиотеки глубокого обучения Caffe (за несколько месяцев до первого публичного релизха TensorFlow). \n",
    "\n",
    "Эта техника быстро стала интернет сенсацией благодаря \"триповым\" картинкам, которые она генерировала, полные алгоритмических артефактов.\n",
    "\n",
    "Пример выходного изрображения Deep Dream:\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/deepdream_example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм Deep Dream почти полностью идентичен технике визуализации для фильтров свертки: запуск сверточной сети в обратном порядке: т.е. делать \"градиентное восхождение\" (gradient ascent) на входе в сверточную сеть чтобы максимизировать активацию конкретного фильтра на верхнем слое сверточной сети. \n",
    "\n",
    "Deep dream использует сл. различия:\n",
    "* С помощью Deep Dream мы стараемся максимизировать активацию целых слоев, а не отдельного фильтра, тем самым одновременно смешивая вместе визуализацию большого числа атрибутов;\n",
    "* Мы начинаем не с пустого, слегка зашумленного ввода, а с уже суествующего изображения - таким образом, полученные визуализации атрибутов будут \"заперты\" в уже существующие визуальные паттерны, искажая элементы изображенияя несколько артистичным образом;\n",
    "* входное изображение обрабатывается в нескольких \"шкалах\" (называемых октавами), что улучшает качество визуализации.\n",
    "\n",
    "Итак, смоделируем собственный Deep Dreams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация Deep Dreams в Keras\n",
    "\n",
    "Мы начнем с предобученной сверточной сети, обученной на ImageNet. В Keras доступно несколько конволюционных сетей:\n",
    "* VGG16\n",
    "* VGG19\n",
    "* Xception\n",
    "* ResNet50\n",
    "* ...\n",
    "хоя один и тот же процесс возможен с любым из них, выбор сверточной сети разумеется повлияет на визуализацию, поскольку разные архитектуры сверточных сетей дают различные обученные атрибуты. Сверточная сеть, используемая в оригинальном Deep Dream была моделью Inception, и известно что на практике Inception порождает очень интересные Deep Dreams, поэтому будем использовать InceptionV3 модель, которая поставляется с Keras.\n",
    "\n",
    "Загрузка предобученной модели InceptionV3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87793664/87910968 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "\n",
    "# We will not be training our model,\n",
    "# so we use this command to disable all training-specific operations\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# Build the InceptionV3 network.\n",
    "# The model will be loaded with pre-trained ImageNet weights.\n",
    "model = inception_v3.InceptionV3(weights='imagenet',\n",
    "                                 include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем вычислим \"потерю\", мы хотим максимизировать во время процесса градиентного спуска (на самом деле, восхождения градиента - gradient ascent). \n",
    "\n",
    "Для визуализации фильтров пытаются максимизировать значение конкретного фильтра на конкретном слое. В случае Deep Dream мы будет одновременно максимизировать активацию всех фильтров на нескольких слоях. В частности, мы будем максимизировать взвешенную сумму $L_2$-нормы активаций множества слоев высокого уровня. Точный набор слоев, которые мывыбираем (а также их вклад в финальную потерю ) имеет большое влияние на визуальные эффекты, поэтому мы можем легко настраивать эти параметры. Более низкоуровневы слои приводят к геометрическим шаблонам, а более высокоуровневые слои  приводят к визуальным эффектам, в которых мвы можете распознавать некоторые классы из ImageNet (например, птиц, или собак).\n",
    "\n",
    "Мы начнем с некоторой произвольной конфигурации,включающей 4 слоя.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dict mapping layer names to a coefficient\n",
    "# quantifying how much the layer's activation\n",
    "# will contribute to the loss we will seek to maximize.\n",
    "# Note that these are layer names as they appear\n",
    "# in the built-in InceptionV3 application.\n",
    "# You can list all layer names using `model.summary()`.\n",
    "layer_contributions = {\n",
    "    'mixed2': 0.2,\n",
    "    'mixed3': 3.,\n",
    "    'mixed4': 2.,\n",
    "    'mixed5': 1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим тензор, который содержит функцию потерь, т.е. взвешенную сумму $L_2$ нормы активаций слоев, приведенных выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# Define the loss.\n",
    "loss = K.variable(0.)\n",
    "for layer_name in layer_contributions:\n",
    "    # Add the L2 norm of the features of a layer to the loss.\n",
    "    coeff = layer_contributions[layer_name]\n",
    "    activation = layer_dict[layer_name].output\n",
    "\n",
    "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
    "    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
    "    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP:Разница между gradient descent и gradient ascent https://stats.stackexchange.com/questions/258721/gradient-ascent-vs-gradient-descent-in-logistic-regression\n",
    "\n",
    "Запускаем gradient ascent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This holds our generated image\n",
    "dream = model.input\n",
    "\n",
    "# Compute the gradients of the dream with regard to the loss.\n",
    "grads = K.gradients(loss, dream)[0]\n",
    "\n",
    "# Normalize gradients.\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
    "\n",
    "# Set up function to retrieve the value\n",
    "# of the loss and gradients given an input image.\n",
    "outputs = [loss, grads]\n",
    "fetch_loss_and_grads = K.function([dream], outputs)\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1]\n",
    "    return loss_value, grad_values\n",
    "\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value > max_loss:\n",
    "            break\n",
    "        print('...Loss value at', i, ':', loss_value)\n",
    "        x += step * grad_values\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала мы определяем список \"scales\" (octaves), при которых мы будем обрабатывать изображения.\n",
    "\n",
    "Каждая последующая шкала больше чем предыдущая с фактором 1.4 (т.е. на 0% больше): мы начинаем с обработки небольшого изображения и непрерывно масштабируем его:\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/deepdream_process.png)\n",
    "\n",
    "Затем, для каждого последующей октавы (octave) с наименьшей до наибольшей, мы запускаем gradient ascent для максимизации loss, который мы ранее определили на этой октаве (at that scale/octave).\n",
    "\n",
    "После каждого запуска gradient ascent, мы перемасштабируем результирующее изображение на 40%.\n",
    "\n",
    "Чтобы избежать потери множества деталей изображения, после каждого последуюшего масштабирования (в результате чего получаются все более размытые и пиксельные изображения), применяем простой трюк: после каждого масштабирования, мы врзвращаем потерянные детали обратно в изображение, что возможно из-за того, что мы знаем как должно выглядеть исходное изображение в более широком масштабе. Если задано небольшое изображение S и большое изображение L, мы можем вычислить разницу между исходным изображением (предполагаемым размером большее чем L), измененным до размера L и исходным размером измененным до размера S - эта разница измеряет детали, потерянные при переходе от S к L.\n",
    "\n",
    "\n",
    "(Source text: To avoid losing a lot of image detail after each successive upscaling (resulting in increasingly blurry or pixelated images), we leverage a simple trick: after each upscaling, we reinject the lost details back into the image, which is possible since we know what the original image should look like at the larger scale. Given a small image S and a larger image size L, we can compute the difference between the original image (assumed larger than L) resized to size L and the original resized to size S — this difference quantifies the details lost when going from S to L.)\n",
    "\n",
    "Также потребуется ряд вспомогательных функций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1,\n",
    "               float(size[0]) / img.shape[1],\n",
    "               float(size[1]) / img.shape[2],\n",
    "               1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "\n",
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures\n",
    "    # into appropriate tensors.\n",
    "    img = image.load_img(image_path)\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a tensor into a valid image.\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((x.shape[1], x.shape[2], 3))\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255.\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image shape (382, 573)\n",
      "...Loss value at 0 : 1.93002\n",
      "...Loss value at 1 : 2.37952\n",
      "...Loss value at 2 : 2.96466\n",
      "...Loss value at 3 : 3.60587\n",
      "...Loss value at 4 : 4.24773\n",
      "...Loss value at 5 : 4.89504\n",
      "...Loss value at 6 : 5.55888\n",
      "...Loss value at 7 : 6.19807\n",
      "...Loss value at 8 : 6.85785\n",
      "...Loss value at 9 : 7.45587\n",
      "...Loss value at 10 : 8.06581\n",
      "...Loss value at 11 : 8.65878\n",
      "...Loss value at 12 : 9.21806\n",
      "...Loss value at 13 : 9.8288\n",
      "Processing image shape (535, 803)\n",
      "...Loss value at 0 : 3.27045\n",
      "...Loss value at 1 : 4.5954\n",
      "...Loss value at 2 : 5.6864\n",
      "...Loss value at 3 : 6.65935\n",
      "...Loss value at 4 : 7.59178\n",
      "...Loss value at 5 : 8.48903\n",
      "...Loss value at 6 : 9.35136\n",
      "Processing image shape (750, 1125)\n",
      "...Loss value at 0 : 3.32889\n",
      "...Loss value at 1 : 4.67524\n",
      "...Loss value at 2 : 6.0011\n",
      "...Loss value at 3 : 7.5503\n",
      "...Loss value at 4 : 9.6593\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Playing with these hyperparameters will also allow you to achieve new effects\n",
    "\n",
    "step = 0.01  # Gradient ascent step size\n",
    "num_octave = 3  # Number of scales at which to run gradient ascent\n",
    "octave_scale = 1.4  # Size ratio between scales\n",
    "iterations = 20  # Number of ascent steps per scale\n",
    "\n",
    "# If our loss gets larger than 10,\n",
    "# we will interrupt the gradient ascent process, to avoid ugly artifacts\n",
    "max_loss = 10.\n",
    "\n",
    "# Fill this to the path to the image you want to use\n",
    "base_image_path = './/CatsVSDogs//IMG_5103.jpg'\n",
    "\n",
    "# Load the image into a Numpy array\n",
    "img = preprocess_image(base_image_path)\n",
    "\n",
    "# We prepare a list of shape tuples\n",
    "# defining the different scales at which we will run gradient ascent\n",
    "original_shape = img.shape[1:3]\n",
    "successive_shapes = [original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "\n",
    "# Reverse list of shapes, so that they are in increasing order\n",
    "successive_shapes = successive_shapes[::-1]\n",
    "\n",
    "# Resize the Numpy array of the image to our smallest scale\n",
    "original_img = np.copy(img)\n",
    "shrunk_original_img = resize_img(img, successive_shapes[0])\n",
    "\n",
    "for shape in successive_shapes:\n",
    "    print('Processing image shape', shape)\n",
    "    img = resize_img(img, shape)\n",
    "    img = gradient_ascent(img,\n",
    "                          iterations=iterations,\n",
    "                          step=step,\n",
    "                          max_loss=max_loss)\n",
    "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
    "    same_size_original = resize_img(original_img, shape)\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "\n",
    "    img += lost_detail\n",
    "    shrunk_original_img = resize_img(original_img, shape)\n",
    "    save_img(img, fname='dream_at_scale_' + str(shape) + '.png')\n",
    "\n",
    "save_img(img, fname='final_dream.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку оригинальная сеть InceptionV3 обучена распознавать изображения размера 299x299 и учитывая что процесс включает уменьшение масштаба изображений с reasonable factor, наша реализация Deep Dream будет порождать гораздо лучшие результаты на изображениях между 300x300 - 400x400.\n",
    "Но несмотря на это, по прежнему можно использовать один и тот же код на изображениях любого размера и любого соотношения.\n",
    "\n",
    "Исходное изображение:\n",
    "![](CatsVSDogs\\IMG_5103.jpg)\n",
    "\n",
    "Deep Dream: \n",
    "![](CatsVSDogs\\dream_at_scale.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследуйте, что можно сделать,отрегулировав какие слои использовать в функции потерь. Слои, которые ниже в сети содержат больше локальных мене абстрактных представлений и приведут к более геометрически выглядящих моделей Deep Dream. Высокоуровневые слои приведут к более узнаваемым визуальным шаблонам, основанным на наиболее распространенных объектах ImgeNet. \n",
    "\n",
    "Вы можете использовать произвольное генерирование параметров в словаре\n",
    "    layer_contributions\n",
    "чтобы быстро исследовать множество различных комбинаций слоев.\n",
    "\n",
    "Примеры полученные с использованием разных конфигураций слоев:\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/range_of_deep_dreams.png)\n",
    "\n",
    "**Резюме**\n",
    "* Deep Dream заключается в запуске сети \"в обратном порядке\" для генерации входных данных, основанных на репрезентациях, обученных сверточной нейронной сетью.\n",
    "* Результаты являются... интересными и имеют некоорое сходство с визуальными артефактами, которые имеют люди с нарушением зрительной коры головного мозга, или употребляющих различные вещества.\n",
    "* Обратите внимание, процесс не является специфичным для моделей изображения или сверточных сетей, можно применить для речи, музыки и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронный перенос стиля (Neural Style Transfer)\n",
    "\n",
    "Помимо Deep Dream, еще одной разработкой в deep learning-drive image modification, представленной летом 2015 - нейронные перенос стиля (Leon Gatys et al.).\n",
    "Алгоритм нейронного переноса стиля имеет много уточнений и породил много вариаций, в том числе вирусное приложения длс смартфонов Prisma. Для простоты, эта секция фокусируется на формулировки из оригинальной стать.\n",
    "\n",
    "Нейронные перенос стиля заключается в применении \"стиля\" референсного изображения к целевому изображению, во время конвертации \"контента\" целевого изображения:\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/style_transfer.png)\n",
    "\n",
    "Под \"стилем\" подразумеваются, по существу текступы, цвета и визуальные шаблоны, в то время как \"контент\" - высокоуровневая макроструктура изображения.\n",
    "\n",
    "Идея переноса стиля тесно связанная с генерацией текстур имела долгую историю в сообществе, занимающихся обработкой изображений еще до разработки нейронного переноса стилей в 2015 году. Однако, оказалось, что реализации основанные на deep learning предлагают результаты, не имеющие аналогов в том, что можно было бы достичь с помощью классических технологий computer vision.\n",
    "\n",
    "Ключевая идея стоящая за реализацией переноса стиля та же самая - центральная для всех алгоритмов глубокого обучения: мы определяем специфичную для задачи функцию потерь, и минимизируем её потери. Мы знаем что хотим достичь: сохранить \"контент\" оригинального изображения, и адаптировать\"стиль\" референсного изображения. Если мы можем математически определить концепты стиля и контента, тогда соответствующая функция потерь может иметь сл. структуру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = distance(style(reference_image) - style(generated_image)) +\n",
    "       distance(content(original_image) - content(generated_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distance - функция нормировки, например $L_2$ норма, *content* - функция, принимающая на вход изображение и вычисляет репрезентацию его контента, а *style* - функция которая принимает изображение и вычисляет репрезентацию его стиля.\n",
    "\n",
    "Минимизация этой функции потерь приводит к тому, что *style(generated_image)* будет близкой к *style(reference_image)*, в то время как *content(generated_image)* будет близкой к *content(original_image)*.\n",
    "\n",
    "Фундаментальное наблюдение, сделанное Gatye et al, что сверточная глубокая сеть предлагает весьма точный способ математически определить style и content функции.\n",
    "\n",
    "### Content loss\n",
    "\n",
    "Активации ранних слоев в сети соответствуют локальной информации об изображении, в то время как активации поздних слоев, содержат все более глобальную и абстрактую информацию. Иначе сформулировав, активации разных слоев сверточной нейронной сети предоставляют декомпозицию контента изображения над разными пространственными шкалами (different spatial scales). Когда мы ожидаем \"контент\" изображения, который является более глобальным и более абстрактным, мы захватываем репрезентации с верхнего слоя конволюционной сети.\n",
    "\n",
    "Таким образом, хорошим кандидатом на функцию потерь для контента будет рассмотрение предобученной сверточной сети и определение нашей потери как $L_2$ нормы активациями верхних слоев, вычисленных над целевым изображением и активациями того же слоя, вычисленного над сгенерированным изображением. Это гарантировало бы, что как видно с верхних слоев сверточной сети, сгенерированное изображние будет очень похоже на оригинальное целевое изображение.\n",
    "\n",
    "Предполагая, что то, что видят верхние слои сверточной сети действительно является \"контентом\" входного изображения, и тогда это работает как способ сохранение контента изображения.\n",
    "\n",
    "### Style loss\n",
    "\n",
    "В то врея как потеря контента обрабатывает верхний слой, потеря стиля как определено у Gatys et al. обрабатывается несколькими слоями сверточной сети: мы стремимся \"хахватить\" внешний вид стиля референсного изображения во всех его пространственных шкалах (spatial scales), извлеченных сверточной сетью, а не только какой-либо одной шкалой.\n",
    "\n",
    "Для функции потерь стиля (style loss), Gatys использовал матрицу Грама слоев активации, то есть скалярное произведение между картами атрибутов (feature maps) заданного слоя. Это скалярное произведение можно понимать как представление отображения корреляций между атрибутами слоя. Эти корреляции атрибут захватывают статистики паттернов конкретной пространственной шкалы (spatial scale), что эмпирически соответствует появлению текстур обнаруженных в этой шкале.\n",
    "\n",
    "Итак, потеря стиля направлена на сохранение подобных внутренних корреляций в активациях разных слоев, по стилю референсного изображения и генерированного изображения. В свою очередь, это гарантирует что текстуры найденные в различных пространственных шкалах (spatial scales) будут выглядеть примерно одинаково в референсном изображении со стилем и генерированнои ищображении.\n",
    "\n",
    "### In short\n",
    "Итак, мы используем предобученную сверточную нейронную сеть для получения потери, которая будет:\n",
    "* Сохранять контент, поддерживая аналогичные активации высокого уровня между целевым изображением контента и сгенерированным изображением. Сверточная сеть должна \"видеть\" какцелевое изображение, так и сгенерированное изображение как \"содержащее одни и те же вещи\".\n",
    "* Сохранить стиль, поддерживая аналогичные корреляции среди активаций как слоев нижнего уровня, так и высокоуровневых слоев. В действительности, корреляции атрибутов захватывают текстуры: сгенерированнок и референснок изображение должно разделять одни и те же текстуры на разных пространственных шкалах (осях) (different spatial scales).\n",
    "\n",
    "### Реализация Neural Style Transfer в Keras\n",
    "Нейронный перенос стиля может быть реализован с использованием любой предобученной сверточной сети. Мы будем использовать VGG19 (которая использовалась Gatys в его статье). VGG19 - простой вариант VGG16 (которую мы изучали ранее), с 3 дополнительными сверточными слоями.\n",
    "\n",
    "Общий процесс:\n",
    "* \"Поднять\" сеть, которая будет вычислять активации слоев VGG19 для стиля референсного изображения целевого изображения и сгенерированного изображения в одно и то же вреия.\n",
    "* Используя активации слоя, вычисленные над этими тремя изображениями, определить функцию потерь, описанную выше, которая минимизируется для достижения переноса стиля.\n",
    "* Настроить процесс градиентного спуска для минимизации функции потерь.\n",
    "\n",
    "Определим пути к изображеним:\n",
    "* референсному изображению со стилем;\n",
    "* целевому изображению.\n",
    "\n",
    "Чтобы убедиться чтообработанные изображения имеют примерно одинаковые размеры (сильно варьирующиеся размеры затруднят перенос стиля), мы проведем их масштабирование на общую высоту в 400 пикселей.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# This is the path to the image you want to transform.\n",
    "target_image_path = './/CatsVSDogs//IMG_5103.jpg'\n",
    "# This is the path to the style image.\n",
    "style_reference_image_path = './CatsVSDogs/style_reference.jpeg'\n",
    "\n",
    "# Dimensions of the generated picture.\n",
    "width, height = load_img(target_image_path).size\n",
    "img_height = 400\n",
    "img_width = int(width * img_height / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам также необходимы некоторые вспомогательные функции для загрузки, предобработки и постобработки, для изображений, которые будут входить и выходить из VGG19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь \"развернем\" VGG19. Она принимает на вход пакет (batch) из 3 изображений: \n",
    "* референсного изображения стиля;\n",
    "* целевого изображения;\n",
    "* placeholder для сгенерированного изображения.\n",
    "\n",
    "Placeholder - это просто символический тензор, значения которого будут предоставлены внешне, через массивы NumPy. Референсное изображение стиля и целевого изображения статические и поэтому определяются с использованием K.constant, где значения содержат placeholder сгенерированного изображения, которое будет меняться с течением времени.\n",
    "\n",
    "Загрузка предобученной VGG19 и применение к 3 изображениям:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
    "\n",
    "# This placeholder will contain our generated image\n",
    "combination_image = K.placeholder((1, img_height, img_width, 3))\n",
    "\n",
    "# We combine the 3 images into a single batch\n",
    "input_tensor = K.concatenate([target_image,\n",
    "                              style_reference_image,\n",
    "                              combination_image], axis=0)\n",
    "\n",
    "# We build the VGG19 network with our batch of 3 images as input.\n",
    "# The model will be loaded with pre-trained ImageNet weights.\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                    weights='imagenet',\n",
    "                    include_top=False)\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим *content loss*, чтобы убедиться что верхний слой сверточной сети VGG19 будет иметь аналогичный вид для целевого и для сгенерированного изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим *style loss*, она обрабатывает вспомогательную функцию для вычисления матрицы Грама (Gram matrix of input matrix) входной матрицы, то есть отображение корреляций обнаруженных в оригинальной матрице атрибутов (original feature matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К этим двум компонентам мы добавляем третий: общая потеря вариаций (total variation loss). Это означает что мы поощряем пространственну непрерывность в сгенерированном изображении и стремимся избежать чрезмерно неровных результатов. \n",
    "\n",
    "Это можно интерпретировать как своеобразную регуляризацию потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] - x[:, 1:, :img_width - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] - x[:, :img_height - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потеря, которую мы минимизируем взвешенное среднее всех этих трех потерь. Для вычисления потери контента, мы обрабатываем только верхний слой (*block5_conv2*), в то вреия как для потери стиля мы охватываем как низкоуровневые так и высокоуровневые слои.\n",
    "В конце концов, мы добавляем общую потерю вариаций.\n",
    "\n",
    "В зависимости от референсного изображения стиля и контентного изображения, которое мы используем, вы вероятно захотите настроить коэффициент content_weight кожффициент, вклад функции потерь контента в общую функцию потерь.\n",
    "\n",
    "Высокое значеине content_weight означает что целевой контент будет более узнаваемым в генерируемом изображении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dict mapping layer names to activation tensors\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "# Name of layer used for content loss\n",
    "content_layer = 'block5_conv2'\n",
    "# Name of layers used for style loss\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "# Weights in the weighted average of the loss components\n",
    "total_variation_weight = 1e-4\n",
    "style_weight = 1.\n",
    "content_weight = 0.025\n",
    "\n",
    "# Define the loss by adding all components to a `loss` variable\n",
    "loss = K.variable(0.)\n",
    "layer_features = outputs_dict[content_layer]\n",
    "target_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss += content_weight * content_loss(target_image_features,\n",
    "                                      combination_features)\n",
    "for layer_name in style_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(style_layers)) * sl\n",
    "loss += total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, конфигурируем процесс градиентного спуска. В оригинальной работе Garys et al. он использовал алгоритм L-BFGS.\n",
    "\n",
    "В этом ключевое отличие от Deep Dreamю L-BFGS - пакет в составе SciPy. Однако, есть два небольших ограничения в реализации SciPy:\n",
    "* Он требует передачи значения целевой функции и значения градиента как две раздельные функции;\n",
    "* Он может быть применен только к плоским векторам, в то время как мы имеем 3 мерный массив изображения.\n",
    "\n",
    "Для нас будет вычислительно неэффективно вычислять значение функции потерь и значение её градиентов независимо, поскольку это приведет к значительным избыточным вычислениям. Мы бы получили замедление почти в два раза, чем при совместном их вычислении. Чтобы передать их, мы напишем класс Evaluator, который будет вычислять значение функции потерь и значение градиентов за раз, и мы возвращаем значение функции потерь, которое вычисляется в первый раз, а затем будем кэшировать градиенты для сл. вызова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the gradients of the generated image wrt the loss\n",
    "grads = K.gradients(loss, combination_image)[0]\n",
    "\n",
    "# Function to fetch the values of the current loss and the current gradients\n",
    "fetch_loss_and_grads = K.function([combination_image], [loss, grads])\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, img_height, img_width, 3))\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем процесс gradient ascent с использованием L-BFGS из SciPy, сохраняя текущее сгенерированное изображение на каждой итерации алгоритма (ниже одна итерация представляет 20 шагов gradient ascent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "import time\n",
    "\n",
    "result_prefix = 'my_result'\n",
    "iterations = 20\n",
    "\n",
    "# Run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss.\n",
    "# This is our initial state: the target image.\n",
    "# Note that `scipy.optimize.fmin_l_bfgs_b` can only process flat vectors.\n",
    "x = preprocess_image(target_image_path)\n",
    "x = x.flatten()\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x,\n",
    "                                     fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    # Save current generated image\n",
    "    img = x.copy().reshape((img_height, img_width, 3))\n",
    "    img = deprocess_image(img)\n",
    "    fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "    imsave(fname, img)\n",
    "    end_time = time.time()\n",
    "    print('Image saved as', fname)\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример который мы получили:\n",
    "\n",
    "--ЗДЕСЬ ПРИМЕР--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Content image\n",
    "plt.imshow(load_img(target_image_path, target_size=(img_height, img_width)))\n",
    "plt.figure()\n",
    "\n",
    "# Style image\n",
    "plt.imshow(load_img(style_reference_image_path, target_size=(img_height, img_width)))\n",
    "plt.figure()\n",
    "\n",
    "# Generate image\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые результаты:\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/revised_style_transfer_results.png)\n",
    "\n",
    "Следует помнить, что то, чего достигает эта техника - это просто форма реструктуризации изображения или передачи текстуры. Она будет работать дучше всего с референсными изображениями стиля очень сильно текстурированы и очень похожи друг на друга и целевой контент не требует высокого уровня детализации для распознавания. Как правило, он не может достичь достаточно абстрактных умений, таких как \"перенос стиля одного портрета на другой\". По сути этот алгоритм ближе к обработке сигналов, поэтому не нужно от него ждать волшебства.\n",
    "\n",
    "Выплнение алгоритма переноса стиля происходит крайне медленно. Однако, преобразование, управляемое нашим кодом достаточно простое и его можно выполнить с помощью небольшой быстрой сверточной сети - разумеется, если у нас есть соответственные обучающие данные. Быстрый перенос стиля может быть достигнут за счет значительных вычислительных трат в первое время для огромного числа циклов вычислений с целью генерации входных-выходных тренировочных примеров для фиксированного референсного изображения тиля с использованием вышеприведенного метода, и затем обучение сверточной сети для обучения сепцифичных для стиля преобразований. Как только это выполнено, стилизация будет происходить достаточно быстро - это просто forward pass небольшой сверточной сети.\n",
    "\n",
    "**Резюме**\n",
    "\n",
    "* Перенос стиля состоит в создании нового изображения, сохраняющео \"контент\" целевого изображения и захватывающий \"стиль\" референсного изображения;\n",
    "* Контент может захватываться высокоровневыми активациями сверточной сети;\n",
    "* \"Стиль\" может захватываться внутренними корреляциями активаций разных слоев сверточной сети;\n",
    "* Deep learning позволяет сформулировать перенос стиля как процесс оптимизации с использованием функции потерь. определенной с помощью предобученной сверточной сети;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация изображение с Variational Autoencoders\n",
    "\n",
    "Сэмплирование из латентного пространства изображений (latent space of images) для создания полностью новых изображений, или редактирование уже существующих - достаточно популярная технгика.\n",
    "\n",
    "Рассмотрим генерацию изображений с помощью двух технологий:\n",
    "* Variational Autoencoders (VAE)\n",
    "* Generative Adversarial Networks (GAN);\n",
    "Эти техники не ограничены только изображениями - они могут применяться к музыке, звукам и даже к тексту. Но на практике самые интересные результаты на данный момент получены на изображениях, на чем мы и сфокусируемся.\n",
    "\n",
    "### Sampling from latent spaces of images\n",
    "\n",
    "Ключевая идея генерации изображений состоит в разработке латентного пространства репрезентаций малой размерности (low-dimensional latent space of representations), которое по своей природе является векторным пространством, где каждая точка может быть отображена на реалистичное изображение. Модуль, способный реализовать это отображение, берет в качестве входа точку в латентном пространстве и порождает изображение, т.е. сетку пикселей, называемую генератором (в случае GAN) или декодером (в случае VAE). Как только такое латентное пространство будет разработано, то можно сэмплировать точки из него или наугад, или путем сопоставления их с пространством изображений, генерируя изображения, которые ранее не встречались (в тестовой выборке).\n",
    "\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/image_generation.png)\n",
    "\n",
    "GAN и VAN просто 2 различные стратегии для обучения таким латентным пространствам репрезентаций изображений, каждая со своими собственными характеристиками.\n",
    "\n",
    "VAE хорошо подходит для обучения латентным пространствам, которые хорошо структурированы, где конкретные направления кодируют значимую ось изменения (вариации) данных (where specific directions encode a meaningful axis of variation in the data).\n",
    "\n",
    "GAN генерирует изображения которые могут быть потенциально высоко реалистичными, но порождаемое ими латентное пространство может не иметь такой структуры и непрерывности.\n",
    "\n",
    "Непрерывное пространство изображений лиц, с использованием VAE:\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/face_space_dribnet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept vectors for image editing\n",
    "\n",
    "Идея вектора концепта уже встречалась в теме, связанной с погружениями слов. Здесь идея остается той же самой: дано латентное пространство репрезентаций (или пространство погружений), где каждое направление в пространстве может кодировать различные оси вариаций в исходных данных. В латентном пространстве изображений лиц, например, может быть 'smile vector' s, такой что если скрытая точка z в погружении репрезентует определенное лицо, то точка z+s является репрезентацией погруженияя этого же лица, но улыбающегося.\n",
    "\n",
    "Как только вы идентифицировали такой вектор, становится возможным редактировать изображения, проецируя их в скрытое пространствво, перемещая их представление осмысленным способом, а затем декодируя их обратно в пространство изображений. Векторы концепций существуют практически для любой независимой размерности в вариациях в пространстве изображений, - в случае изображений лиц, можно обнаружить векторы для добавления солнцезащитных очков к лицу, удаления очков, поворота мужского лица в женское и т.д.\n",
    "\n",
    "Пример \"smile vector\", вектора концепта (concept vector), который был исследоватен Tom White с использованием VAE на наборе данных лиц знаменитостей (CelebA dataset):\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/smile_vector.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational autoencoders (VAE)\n",
    "\n",
    "VAE, одновременно исследовались Kingma & Welling (December 2013) и Rezende, Mohamed & Wierstra (January 2014) как вид генеративной модели, который особенно соответствует для задачи редактирования изображений через вектора концепций. Они являются современным подходом к автокодировщикам (Autoencoder) - тип сети, цель которой состоит в \"кодировании\" входа в латентное пространство малой размерности, а затем обратное декодирование  - которое смешивает идет deep learning с Байесовским выводом.\n",
    "\n",
    "Классические автокодировщики изображений берут вхож, отображают его в латентное векторное пространство через модуль кодировщика (encoder), затем декодируют его обратно в вывод, с той же размерностью, что и оригинальное изображенгие, но через модуль декодирования. Затем его обучают, используя в качестве целевых данных те же изображения, что и входные изображения - а это означает, что автокодировщик учиться реконструировать оригинальный вход. Накладывая различные ограничения на \"код\", т.е. выход кодировщика (encoder), мы можем обучить автокодировщик обучать более или менее интересные латентные репрезентации данных. Чаще всего один из них ограничивает код, чтобы он имел низкую размерность и в этом случае кодировик выступает как метод компрессии входных данных.\n",
    "\n",
    "Автокодировщик: отображение входа $x$ в сжатую репрезентацию, а затем обратное декодирование в $x'$.\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/autoencoder.jpg)\n",
    "\n",
    "На практике такие классические автокодировщики не приводят к особо полезным, или хорошо структурированным латентным пространствам. Они не особо хороши  в компрессии. По этим причинам они в последние годы стали выходить из моды. Вариационные автокодировщики, однако, дополняют автокодировщики небольшим количеством statistical magic, заставляя их изучать непрерывные структурированные латентные пространства. Они оказались очень мощным инструментом генерации изображений.\n",
    "\n",
    "VAE, вместо компрессии входного изображения в некоторый фиксированный \"код\" в латентном пространстве преобразует изображение в его параметры статистического распределения: математическое ожидание и дисперсию.\n",
    "\n",
    "По сути это означает, что мы допускаем что входное изображение генерируется статистическим процессом и что случайность этого процесса должна учитываться во время кодирования\\декодирования VAE затем использует матожидание и дисперсию для случайного сэмплирования одного элемента из распределения и декодирует этот элемент в оригинальный вход. Стохастичесность этого процесса улучшает робастность и заставляет латентное пространство кодировать осмысленные репрезентации для каждой точки, так что сэмплируемая точка из латентого пространства будет декодирована в корректный выход.\n",
    "\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/vae.png)\n",
    "\n",
    "В технических терминах, VAE работает сл. образом. Сначала, модуль кодировщика преобразует входные изображения *input_img* в два параметра в латентном пространстве репрезентаций, которые будет обозначать *z_mean* и *z_log_variance*. Заьем, случайно сэмплирем точку *z* из латентного нормального распределения, так что предполагается генерация входного изображения: *z = z_mean + exp(z_log_variance) x epsilon*,где *epsilon* - случайные тензор малых значений. Наконец, модуль декодера будет отображать точку в латентном пространстве обратно в оригинальное входное изображение. Поскольку *epsilon* - случаное, процесс гарантирует, что каждая точка близка к латентной локации, где мы кодируем *input_img* (*z-mean*) может быть декодирована в нечто подобное к *input_img*, что тем самым вынуждая латентное простраснтво быть непрерывно осмысленным. Любые две близкие точки в латентном пространстве декодируются в очень похожие изображения.  \n",
    "\n",
    "Непрерывность, в сочетании с низкой размерностью латентного пространства заставляет каждое направление латентного пространства кодировать значимые оси вариаций данных, делая латентное пространство структурированным и поэтому подходящим для манипуляций с помощью векторов концептов.\n",
    "\n",
    "Параметры VAE обучаются через 2 функции потерь: первая реконструирует потери, которые заставляют декодерованные сэмплы соответствовать начальным входам, и регуляризацию потери, которая помогает в обучении адекватного латентного пространства и уменьшает переобучение на тренировочных данных.\n",
    "\n",
    "Схематическая форма VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode the input into a mean and variance parameter\n",
    "z_mean, z_log_variance = encoder(input_img)\n",
    "\n",
    "# Draw a latent point using a small random epsilon\n",
    "z = z_mean + exp(z_log_variance) * epsilon\n",
    "\n",
    "# Then decode z back to an image\n",
    "reconstructed_img = decoder(z)\n",
    "\n",
    "# Instantiate a model\n",
    "model = Model(input_img, reconstructed_img)\n",
    "\n",
    "# Then train the model using 2 losses:\n",
    "# a reconstruction loss and a regularization loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим сеть кодировщика, очень простая сверточная сеть которая отображает входное изображение $x$ в два вектора: *z_mean* и *z_log_variance*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "img_shape = (28, 28, 1)\n",
    "batch_size = 16\n",
    "latent_dim = 2  # Dimensionality of the latent space: a plane\n",
    "\n",
    "input_img = keras.Input(shape=img_shape)\n",
    "\n",
    "x = layers.Conv2D(32, 3,\n",
    "                  padding='same', activation='relu')(input_img)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', activation='relu',\n",
    "                  strides=(2, 2))(x)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', activation='relu')(x)\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для использованиея z_mean и z_log_var, параметры статистического распределения, предположительно порожденные input_img, для отображения в точку в латентном пространстве. Мы обернули произвольную функцию в Keras lambda слой. В Keras все предполагается слоем, поэтому для выполнения произвольного кода необходимо обернуть его в слой, это то чем и занимается Lambda слой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация декодера: мы проводим перенормировку вектора z в размерности изображения, затем используется несколько сврточных слое жлдя получения конечного изображения той же размерности, что оригинальное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the input where we will feed `z`.\n",
    "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
    "\n",
    "# Upsample to the correct number of units\n",
    "x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
    "                 activation='relu')(decoder_input)\n",
    "\n",
    "# Reshape into an image of the same shape as before our last `Flatten` layer\n",
    "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
    "\n",
    "# We then apply then reverse operation to the initial\n",
    "# stack of convolution layers: a `Conv2DTranspose` layers\n",
    "# with corresponding parameters.\n",
    "x = layers.Conv2DTranspose(32, 3,\n",
    "                           padding='same', activation='relu',\n",
    "                           strides=(2, 2))(x)\n",
    "x = layers.Conv2D(1, 3,\n",
    "                  padding='same', activation='sigmoid')(x)\n",
    "# We end up with a feature map of the same size as the original input.\n",
    "\n",
    "# This is our decoder model.\n",
    "decoder = Model(decoder_input, x)\n",
    "\n",
    "# We then apply it to `z` to recover the decoded `z`.\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Двойная потеря VAE не соответствует традиционным ожиданиям sample-wise функции в форме loss(input, target). Мы устанавливаем потери, написав слой с внутренней обработкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        kl_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We don't use this output.\n",
    "        return x\n",
    "\n",
    "# We call our custom layer on the input and the decoded output,\n",
    "# to obtain the final model output.\n",
    "y = CustomVariationalLayer()([input_img, z_decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, создаем экземпляр модели и обучаем её. Поскольку о потерях заботится наш собственный слой, мы не указываем внешние потери на фазе компиляции (loss=None), что в свою очередь означает что мы не передаем целевые данные на фазе обучения (как можно заметить, мы пережаем только x_train в модель при обучении (fit))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "vae = Model(input_img, y)\n",
    "vae.compile(optimizer='rmsprop', loss=None)\n",
    "vae.summary()\n",
    "\n",
    "# Train the VAE on MNIST digits\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "vae.fit(x=x_train, y=None,\n",
    "        shuffle=True,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим на MNIST - можно попробовать сеть для декодирования чтобы преобразовать произвольную точку латентного пространства в изображение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# Linearly spaced coordinates on the unit square were transformed\n",
    "# through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z,\n",
    "# since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid of sampled digits shows a completely continuous distribution of the different digit classes, with one digit morphing into another as you follow a path through latent space. Specific directions in this space have a meaning, e.g. there is a direction for \"four-ness\", \"one-ness\", etc.\n",
    "\n",
    "In the next section, we cover in detail the other major tool for generating artificial images: generative adversarial networks (GANs).\n",
    "\n",
    "### Take aways\n",
    "\n",
    "Image generation with deep learning is done by learning latent spaces that capture statistical information about a dataset of images. By sampling points from the latent space, and \"decoding\" them, one can generate never-seen-before images. There are two major tools to do this: VAEs and GANs.\n",
    "\n",
    "VAEs result in highly structured, continuous latent representations. For this reason, they work well for doing all sort of image edition in latent space, like face swapping, turning a frowning face into a smiling face, and so on. They also work nicely for doing latent space based animations, i.e. animating a walk along a cross section of the latent space, showing a starting image slowly morphing into different images in a continuous way.\n",
    "GANs enable the generation of realistic single-frame images, but may not induce latent spaces with solid structure and high continuity.\n",
    "Most successful practical applications I have seen with images actually rely on VAEs, but GANs are extremely popular in the world of academic research — at least circa 2016-2017. You will find out how they work and how to implement one in the next section.\n",
    "\n",
    "To play further with image generation, I suggest working with the CelebA dataset, \"Large-scale Celeb Faces Attributes\". It’s a free-to-download image dataset with more than 200,000 celebrity portraits. It’s great for experimenting with concept vectors in particular. It beats MNIST for sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
