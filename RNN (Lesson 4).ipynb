{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекуррентные нейронные сети (РНС)\n",
    "\n",
    "В сверточных нейронных сетях используется пространственная геометрия данных. Так, операции свертки и пулинга применяются вдоль временной оси для звуковых данных, в двух пространственных измерениях для изображений и в трех измерениях (высота, ширина и время) для видео.\n",
    "\n",
    "**Рекуррентные нейронные сети** - класс нейронных сетей, в которых учитывается последовательный характер входных данных. На вход подается текст, речь, временной ряд или еще какие-либо данные, в которых появление элемента в последовательности зависит от предшествующих элементов.\n",
    "\n",
    "РНС можно представить как граф, состоязий из элементарных ячеек, каждая из которых выполгняет одну и ту же опеарцию для каждого элемента последовательности. РНС обладает большой гибкостью и применяется для решения многих задач:\n",
    "* распознавание речи;\n",
    "* языковое моделирование\n",
    "* машинный перевод\n",
    "* анализ эмоциональной окраски\n",
    "* теггирование изображений\n",
    "* и т.д.\n",
    "\n",
    "РНС можно адаптировать к различным типам задач, изменяя конфигурацию ячеек в графе. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простые ячейки РНС\n",
    "Традиционно, в нейросетях на основе многослойных перцептронов предполагается, что все входны незаивисимы. В последовательных данных это предположение нарушается. \n",
    "Пример: \n",
    "* Текст - первое слово предложения влияет и на второе;\n",
    "* Речь - не расслышав слово можно догадаться какое было слово, исходя из слов который произнес собеседник ранее.\n",
    "* Временные ряды - цены на акции и прогнозы погоды также характерна зависимость от прошлых данных;\n",
    "\n",
    "В ячейках РНС эта зависимсть представляется с помозью скрытого состояния (или памяти), в которой хранится сводка прошлой информации. Значение скрытого состояния в любой момент времени - функция значения на предыдущем шаге и значения данных на текущем шаге: \n",
    "\n",
    "$$ h_t = \\phi(h_{t-1}, x_t)$$\n",
    "\n",
    "* $h_t, h_{t-1}$ - значения скрытого состояния на шаге $t$ и $t-1$ соответственно,\n",
    "* $x_t$ - входное значение в момент времени $t$.\n",
    "\n",
    "Ячейку РНС можно представить графически. В момент $t$ ячейка получает а входе значение $x_t$ и выводит значение $y_t$. Часть $y_t$ (скрытое состояние $h_t$) подается обратно на вход ячейки для использования на сл. шаге $t+1$. Если параметры традиционной нейронной сети хранятся в матрице весов, то параметры РНС задаются тремя матрицами весов $U,V$ и $W$, соответствующими входу, выходу и скрытому состоянию.\n",
    "![](./img/rnn-pic1.png)\n",
    "Еще один взгляд на РНС - *развертка*. Это означает, что мы рисуем сеть на протяжении всей последоватевательности. На рисунке изображена РНС с 3 слоями, пригодная для обработки последовательностей с тремя элементами. Заметим, что матрицы весов $U,V,W$ разделяются **между всеми шагами, поскольку на каждом шаге к разным данным применяются одни и те же операции**. Благодаря использвоанию одних и тех же весов на всех временных шагах удается существенно снизить количество обучаемых параметров РНС.\n",
    "\n",
    "Вычислния выполняемые РНС можно также описать в виде уравнений. Внутреннее состояние РНС в момент времени $t$ определяется значением вектора $h_t$, равного результату применения нелинейности *tanh* к сумме произведения матрицы весов $W$ на скрытое состояние $h_{t-1}$ в момент $t-1$ и произведения матрицы весов $U$ на входное значение $x_t$ в момент $t$. Выбор нелинейности *tanh*, а не какой-то лругой, связан с тем, что её вторая производная очень медленно убывает, приближаясь к нулю, поэтому градиенты остаются в линейной части функции активации, что помогает справится с проблемой **исчезающего градиента**.\n",
    "\n",
    "Выходной вектор $y_t$ в момент времени $t$ равен результату применения функции *softmax* к произведению матрицы весов $V$ на скрытое состояние $h_t$ и представляет набор вероятностей выхода:\n",
    "$$ h_t = tanh(Wh_{t-1}+Ux_{t})$$\n",
    "$$ y_t = softmax(Vh_t)$$\n",
    "\n",
    "Keras предоставляет слой рекуррентной нейронной сети SimpleRNN, включающий всю описанную логику, и эффективные варианты LSTM и GRU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простая РНС с применением Keras - порождение текста\n",
    "\n",
    "РНС активно используется в NLP для решения различных задач. Одна из которых **построения языковых моделей**,. Такая модель позволяет предсказать вероятность появления слова в тексте при условии известных предыдущих слов. Языковые модели важны для таких высокоуровневых прилоежнией, как машинные перевод, исправление правописания и т.д.\n",
    "\n",
    "Побочным эффектим умения предсказать сл. слово по известным предыдущим является порождающая модель, которая генерирует текст путем переборки слов из выходного распределения. В случае языкового моделирования входом обычно является последовательность слов, а вызолом последовательность предсказанных слов. В роли робучающих данных выступает меющийся непомеченный текст, и метка $y_t$ в момент времени $t$ становится входом $x_{t+1}$ в момент времени $t+1$.\n",
    "\n",
    "Первым примером исползования Keras для построения РНС будет языковая модель обученная предсказывать следующий символ по 10 рпредыдущим на тексте \"Алисы в стране чудес\".\n",
    "\n",
    "Мы остановились на модели предсказания символа, потому что у нее меньше словарь и обучение происходит быстрее. Но та же идея применима к предсказанию слов, нужно только символы заменить словами. Обученная модель будет использована для порождения нового текста в том же стиле.\n",
    "\n",
    "Сначала импортируем модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входной текст \"Алисы в Стране чудес\" (на англ. языке) берем с сайта проекта Гутенберг по адресу http://www.gutenberg.org/files/11/11-0.txt\n",
    "\n",
    "Файл содержит символы конца строки и символы не в кодировке ASCII, поэтому произведем предварительную обработку и запишем результат в переменную text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from input...\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"./data/11-0.txt\"\n",
    "\n",
    "# extract the input as a stream of characters\n",
    "print(\"Extracting text from input...\")\n",
    "fin = open(INPUT_FILE, 'r', encoding='utf-8')\n",
    "lines = []\n",
    "for line in fin:\n",
    "    line = line.strip().lower()\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку РНС будет предсказывать символы, то и словарь состоит из множества символов, встречающихся в тексте. Таковых у нас 47. Мы будем иметь дело не с самими символами, а с их индексами, а с индексами, поэтому в сл. фрагменте создаются таблицы соответствия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating lookup tables\n",
    "# Here chars is the number of features in our character \"vocabulary\"\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "char2index = dict((c, i) for i, c in enumerate(chars))\n",
    "index2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 32,\n",
       " '!': 14,\n",
       " '(': 21,\n",
       " ')': 15,\n",
       " '*': 16,\n",
       " ',': 9,\n",
       " '-': 25,\n",
       " '.': 23,\n",
       " '0': 43,\n",
       " '3': 46,\n",
       " ':': 13,\n",
       " ';': 12,\n",
       " '?': 29,\n",
       " '[': 22,\n",
       " ']': 44,\n",
       " '_': 7,\n",
       " 'a': 4,\n",
       " 'b': 11,\n",
       " 'c': 27,\n",
       " 'd': 24,\n",
       " 'e': 33,\n",
       " 'f': 0,\n",
       " 'g': 45,\n",
       " 'h': 28,\n",
       " 'i': 35,\n",
       " 'j': 2,\n",
       " 'k': 30,\n",
       " 'l': 39,\n",
       " 'm': 5,\n",
       " 'n': 3,\n",
       " 'o': 8,\n",
       " 'p': 42,\n",
       " 'q': 1,\n",
       " 'r': 41,\n",
       " 's': 34,\n",
       " 't': 36,\n",
       " 'u': 37,\n",
       " 'v': 6,\n",
       " 'w': 18,\n",
       " 'x': 19,\n",
       " 'y': 20,\n",
       " 'z': 38,\n",
       " '‘': 10,\n",
       " '’': 40,\n",
       " '“': 26,\n",
       " '”': 31,\n",
       " '\\ufeff': 17}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий шаг - создание входных строк и меток. Для этого проходим по тексту с шагом STEP сиволов (в нашем солуча 1) и выделяем отрезки длиной SEQLEN (в нашем случае 10). Следующий после отрезка символ будет меткой:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating input and label text...\n"
     ]
    }
   ],
   "source": [
    "# create inputs and labels from the text. We do this by stepping\n",
    "# through the text ${step} character at a time, and extracting a \n",
    "# sequence of size ${seqlen} and the next output char. For example,\n",
    "# assuming an input text \"The sky was falling\", we would get the \n",
    "# following sequence of input_chars and label_chars (first 5 only)\n",
    "#   The sky wa -> s\n",
    "#   he sky was ->  \n",
    "#   e sky was  -> f\n",
    "#    sky was f -> a\n",
    "#   sky was fa -> l\n",
    "print(\"Creating input and label text...\")\n",
    "SEQLEN = 10\n",
    "STEP = 1\n",
    "\n",
    "input_chars = []\n",
    "label_chars = []\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот код строит из текста последовательность входных строк и меток:\n",
    "   \n",
    "* The sky wa -> s\n",
    "* he sky was ->  \n",
    "* e sky was  -> f\n",
    "*  sky was f -> a\n",
    "* sky was fa -> l\n",
    "   \n",
    "Следующий шаг - векторизация входных строк и меток. На вход РНС подаются построенные выше входные строки. В каждой из них SEQLEN символов, а поскольку размер словаря составляет nb_chars, то каждый входной символ представляется унитарным вектором длины nb_chars. Следовательно, каждый входной элемент представляет собой тензор формы SEQLEN x nb_chars. Выходная метка - это единственный символ , поэтому по аналогии с представлением входных символов она представляется унитарным вектором длины nb_chars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing input and label text...\n"
     ]
    }
   ],
   "source": [
    "# vectorize the input and label chars\n",
    "# Each row of the input is represented by seqlen characters, each \n",
    "# represented as a 1-hot encoding of size len(char). There are \n",
    "# len(input_chars) such rows, so shape(X) is (len(input_chars),\n",
    "# seqlen, nb_chars).\n",
    "# Each row of output is a single character, also represented as a\n",
    "# dense encoding of size len(char). Hence shape(y) is (len(input_chars),\n",
    "# nb_chars).\n",
    "print(\"Vectorizing input and label text...\")\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем строить модель. Размерность выхода РНС пусть равна будет 128. Это гиперпараметр, определяемый в ходе элементов. В общем случае, если выбрать слишком маленькое значение, то емкость модели будет недостаточна для порождения хороших текстов, и мы увидим длинные серии повторяющихся символов или повторябщиеся группы слов. Если значение слишком велико, то у модели будет слишком много параметров, так что для её обучения потребуется гораздо больше данных. Мы хотим получать на выходе один символ, а не последовательность, поэтому задаем параметр return_sequence=False. Входные данные РНС имеют форму матрицы SEQLEN x nb_chars. Кроме того, задается параметр unroll=True, потому что при этом повышается качество работы базовой библиотеки TensorFlow.\n",
    "\n",
    "РНС соединяется с плотным (полносвязным) слоем. В плотном слое nb_char нейронов, которые выдают оценки появления каждого символа из словаря. Функция активации в этом случае является softmax которая нормирует оценки, преобразуя их в вероятности. Символ с наибольшей вероятностью возвращается в качестве предсказания. При компиляыии модели задается категориальная перекрестная энтропия в качестве функции потерь (она хорошо подходит для категориального выхода) и оптимизатор RMSProp:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the model. We use a single RNN with a fully connected layer\n",
    "# to compute the most likely predicted output char\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,\n",
    "                    input_shape=(SEQLEN, nb_chars),\n",
    "                    unroll=True))\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подход к обучению немного отличается от того, что видели прежде. Мы обучали модель в течении фиксированного числа периодов, а затем оценивали её на зарезервированных для этой цели тестовых данных.  Поскольку в данном случае у нас нет помеченных данных, то мы выполняем один период (NUM_EPOCHS_PER_ITERATONS = 1), а затем тестируем модель. Так происходит на протяжении 25 итераций (NUM_ITERATIONS=25). Следовательно, по существу выполяняется NUM_ITERATIONS периодов обучения и тестируем модель после каждого периода.\n",
    "\n",
    "Тестирование производится так: модель порождает символ по заданныи входным данным, затем первый символ входной строки отбрасывается, в конец дописывается предсказанный на предыдущем прогоне символ и у модели запрашивается сл. предсказание. Так повторяется 100 раз (NUM_PREDS_PER_EPOCH=100), после чего получившаяся строка печатается. Эта строка и является индикатором качества модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 29s - loss: 2.3514    \n",
      "Generating from seed: ver with d\n",
      "ver with dout the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 2.0291    \n",
      "Generating from seed:  interest \n",
      " interest the care the charded the charded the charded the charded the charded the charded the charded the cha\n",
      "==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.9241    \n",
      "Generating from seed: ou know.’ \n",
      "ou know.’ ‘i don’t and and all at in a could be and at in a don’t and and all at in a could be and at in a don\n",
      "==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.8466    \n",
      "Generating from seed:  ‘he won’t\n",
      " ‘he won’t the mouse for the the said the mock turtle the mouther all the the moust as it little the mouther a\n",
      "==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.7817    \n",
      "Generating from seed: obster; i \n",
      "obster; i don’t the hither was and the hat the hat the hat the hat the hat the hat the hat the hat the hat the\n",
      "==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 31s - loss: 1.7281    \n",
      "Generating from seed: ee a littl\n",
      "ee a little the har sood the har her she was the the har sood the har her she was the the har sood the har her\n",
      "==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.6822    \n",
      "Generating from seed: ver, she s\n",
      "ver, she said to her foot really to her foot really to her foot really to her foot really to her foot really t\n",
      "==================================================\n",
      "Iteration #: 7\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 29s - loss: 1.6455    \n",
      "Generating from seed: above the \n",
      "above the was a little she was so dound a little she was so dound a little she was so dound a little she was s\n",
      "==================================================\n",
      "Iteration #: 8\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.6100    \n",
      "Generating from seed: their verd\n",
      "their verd it alice the mock turtle while the was she was she was she was she was she was she was she was she \n",
      "==================================================\n",
      "Iteration #: 9\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 25s - loss: 1.5817    \n",
      "Generating from seed: ’ said the\n",
      "’ said the morse for a mare of the morely say way so finded the parre the mouse sat it was she was she was she\n",
      "==================================================\n",
      "Iteration #: 10\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 30s - loss: 1.5550    \n",
      "Generating from seed: d a canary\n",
      "d a canary with a little said to herself the dormouse said the dormouse said the dormouse said the dormouse sa\n",
      "==================================================\n",
      "Iteration #: 11\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 33s - loss: 1.5320    \n",
      "Generating from seed: our nose--\n",
      "our nose--’ ‘then it was so mance said the mock turtle with a little begin a little begin a little begin a lit\n",
      "==================================================\n",
      "Iteration #: 12\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.5117    \n",
      "Generating from seed:  last, the\n",
      " last, the rabbit with the rabbit with the rabbit with the rabbit with the rabbit with the rabbit with the rab\n",
      "==================================================\n",
      "Iteration #: 13\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 25s - loss: 1.4932    \n",
      "Generating from seed:  the roof \n",
      " the roof little she said to the bard when i can the door little got a more the door little got a more the doo\n",
      "==================================================\n",
      "Iteration #: 14\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.4760    - ETA: 0s -\n",
      "Generating from seed: t cried ou\n",
      "t cried out of the court have the caterpillar some of the court have the caterpillar some of the court have th\n",
      "==================================================\n",
      "Iteration #: 15\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.4618    \n",
      "Generating from seed: ‘it all ca\n",
      "‘it all came of the gryphon alice was she had not it was the mock turtle was it was to the gryphon alice was s\n",
      "==================================================\n",
      "Iteration #: 16\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.4473    - ETA: 0s - \n",
      "Generating from seed: ‘i don’t e\n",
      "‘i don’t every hand in the the wind with the words was the words was the words was the words was the words was\n",
      "==================================================\n",
      "Iteration #: 17\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.4355    \n",
      "Generating from seed: pped, and \n",
      "pped, and the said to herself the said to herself the said to herself the said to herself the said to herself \n",
      "==================================================\n",
      "Iteration #: 18\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.4226    \n",
      "Generating from seed: ! ugh, ser\n",
      "! ugh, serpent, who said this sime the with one was not it was the caterpillar, you donot lead no she could no\n",
      "==================================================\n",
      "Iteration #: 19\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.4132    \n",
      "Generating from seed: they were \n",
      "they were the dourt was a little thing it was a little thing it was a little thing it was a little thing it wa\n",
      "==================================================\n",
      "Iteration #: 20\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.4039    \n",
      "Generating from seed:  should it\n",
      " should it the words was a little to the dormouse some crowing to be a herpent, and then they was a little to \n",
      "==================================================\n",
      "Iteration #: 21\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 29s - loss: 1.3945    \n",
      "Generating from seed: . ‘no, no!\n",
      ". ‘no, no! the mock turtle sound of the mock turtle sound of the mock turtle sound of the mock turtle sound of\n",
      "==================================================\n",
      "Iteration #: 22\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 27s - loss: 1.3860    \n",
      "Generating from seed: alice, qui\n",
      "alice, quite a little beruse in a little beruse in a little beruse in a little beruse in a little beruse in a \n",
      "==================================================\n",
      "Iteration #: 23\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.3769    \n",
      "Generating from seed:  your maje\n",
      " your majesty to like the king the gryphon said to herself the rabbit his his his tone, and the gryphon said t\n",
      "==================================================\n",
      "Iteration #: 24\n",
      "Epoch 1/1\n",
      "142635/142635 [==============================] - 26s - loss: 1.3703    \n",
      "Generating from seed: nderstood \n",
      "nderstood about the mouse of the the the mock turtle was the mock turtle was the mock turtle was the mock turt\n"
     ]
    }
   ],
   "source": [
    "# We train the model in batches and test output generated at each step\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "    \n",
    "    # testing model\n",
    "    # randomly choose a row from input_chars, then use it to \n",
    "    # generate text from model for next 100 chars\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "    print(\"Generating from seed: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for i, ch in enumerate(test_chars):\n",
    "            Xtest[0, i, char2index[ch]] = 1\n",
    "        pred = model.predict(Xtest, verbose=0)[0]\n",
    "        ypred = index2char[np.argmax(pred)]\n",
    "        print(ypred, end=\"\")\n",
    "        # move forward with test_chars + ypred\n",
    "        test_chars = test_chars[1:] + ypred\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вначале модель предсказывает вздор, но к концу 25-го периода она пишет почти без ошибок, хотя со связностью мыслей пока дело обстоит неважно. Удивительно что модель обучилась выводить символы и не имеет ни малейшего представления о словах, как будто они взяты из оригинального текста.\n",
    "\n",
    "Порождение следующего символа или слова - не единственное на что способна такая модель. Подобные модели успешно применялись для предсказания цен на акции (Financial Market Time Series Prediction with Recurrent Nural Networks) и для генераци классической музыки (DeepBach: A steerable Model for Bach Chorales Generation).\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Топологии РНС\n",
    "\n",
    "API многослойного перцептрона и сверточной сети ограничены. Обе архитектуры принимают на входе и порождают на выходе тензоры фиксированного размера, а для преобразования входа в выход выполняют фиксированное число шагов, определяемое числом слоев сети. У РНС такого ограничения нет - входом, выходом или тем и другим могут быть поселдовательности. Это означает, что для решения конкретных задач РНС можно конфигурировать разными способами.\n",
    "\n",
    "Как мы знаем, РНС комбинирует входной вектор с предыдущим вектором состояния для получения новго вектора состояния. Это можно рассматривать как аналог выполнения программы с некоторыми входными данными и внутренними переменными. Следовательно, РНС можно считать способом описания компьютерных программ и доказано, что РНС являются полными по Тьюрингу исполнителям (\"On the computational Power of Neural Nets\") в том смысле, что при задании надлежащих весов они могут моделировать произвольные программы.\n",
    "\n",
    "Умение работать с последовательноcтями открывает возможность для различных топологий, некоторые из которых мы рассмотрим.\n",
    "\n",
    "В базовой структуре все входные последовательности имеют одинаковую длину, а выход порождается на каждом временном шаге. Пример уже видели ранее.\n",
    "\n",
    "![](./img/rnn-pic2.png)\n",
    "\n",
    "\n",
    "Другой пример РНС сети типа **многие-ко-многим** - сеть машинного перевода (b), являющаяся представителем общего семейства сетей последовательность-в-последовательность (seq-2-seq) (O. Vinyals \"Grammar as a Foreign Language\"). Они принимают на вход последовательность и порождают другую последовательность.\n",
    "\n",
    "В случае машинного перевода входом может быть, например, последовательность английскиз слов, а выходом - переведенное предложение на испанском языке.\n",
    "\n",
    "Такой же тип модели используется для PoS-тэггинга. От предыдущей топологии эта отличается тем, что в некоторые моменты времени может отсутствовать вход, а в некоторые - выход.\n",
    "\n",
    "Еще один вариант топологии - сеть типа **один-ко-многим** на рисунке (c), римером которой может служить сеть для подписывания изображений (A. Karphathy, F. Li \"DeepVisual-Semantic Alignment for Generating Image Description\"), где вход - изображение, а выход - последовательность слов.\n",
    "\n",
    "Пример сети типа **многие-ко-многим** - сеть анализа эмоциональной окраски предложений, где вход - последовательность слов, а выход - индикатор положительной или отрицательной окраски (R. Socher \"Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank\"). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема исчезающего и взрывного градиента\n",
    "\n",
    "Как в традиционных нейронных сетях, обучение РНС включает обратное распространение ошибки. Различие в том, что поскольку на всех шагах используются одинаковые параметры, то и градиент в кажом выходе зависит не только от текущего временного шага, но и от предыдущих.\n",
    "\n",
    "Этот процесс называется **обратным распространением во времени (backpropagation through time, BPTT)**.\n",
    "\n",
    "\n",
    "![](./img/rnn-pic3.png)\n",
    "\n",
    "Рассмотрим небольшую трехслойную сеть. В процессе прямого распространения (сплошные линии) сеть порождает предсказания, которые сравниваются с метками для вычисления потери $L_t$ на каждом временном шаге. В процессе обратного распространения (пунктирные линии) на каждом временном шаге вычисляются градиенты функции потерь по параметрам $U,V,W$ и сумма градиентов применяется \n",
    "для обновления параметров. \n",
    "\n",
    "В следующем уравнении показан градиент функции потерь по матрице $W$ в которой закодированы веса для долгосрочных зависимостей. Мы акцентируем внимание на этой части обновления, потому что именно она - причина **проблемы исчезающего и взрывного градиента**. Два других градиента функици потерь по матрицам $U$ и $V$ также суммируются по всем временным шагам:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\sum_t \\frac{\\partial L_t}{\\partial W}$$\n",
    "\n",
    "Посмотрим что происходит с градиентом функции потреь на последнем временном шаге ($t=3$). Этот градиент можно разложить в произведение трех подградиентов, применив правило дифференцирования сложной функции. Градиент скрытого состояния $h_2$ по $W$ можно затем представить в виде суммы градиентов каждого скрытого состояния по предыдущему. Наконец, градиент скрытого состояния по предыдущему можно разложить в произведение градиентов текущего скрытого состояния по предыдущему:\n",
    "\n",
    "$$\\frac{\\partial L_3}{\\partial W} = \\frac{\\partial L_3}{\\partial y_3}\\frac{\\partial y_3}{\\partial h_2}\\frac{\\partial h_2}{\\partial W} = \\sum_{t=0}^{2} \\frac{\\partial L_3}{\\partial y_3}\\frac{\\partial y_3}{\\partial h_2}\\frac{\\partial h_2}{\\partial h_t}\\frac{\\partial h_t}{\\partial W} = \\sum_{t=0}^{2}\\frac{\\partial L_3}{\\partial y_3}\\frac{\\partial y_3}{\\partial h_2}(\\prod_{j=t+1}^{2}\\frac{\\partial h_j}{\\partial h_{j-1}})\\frac{\\partial h_t}{\\partial W} $$\n",
    "\n",
    "Аналогично вычисляются градиенты функции потерь $L_1$ и $L_2$ (на шагах 1 и 2) по $W$, после чего их сумма используется для оьновления градиента по $W$.\n",
    "\n",
    "Последнего выражения градиента в формуле выше достаточно чтобы понять откуда возникает проблема исчезающего и взрывного градиента в РНС. Рассмотрим случай, когда отдельные градиенты скрытого состояния по рпедыдущему меньше 1. При обратном распространении ошибки через несколько временных шагов произведение градиентов становится все меньше и меньше, что и ведет к проблеме исчезающего градиента. С другой стороны, если градиенты больше 1, то произведения растут - вот вам и проблема взрывного градиента.\n",
    "\n",
    "Из-за эффекта исчезающего градиента получается, что градиенты на отдаленных шагах не дают никакого вклада в процесс обучения, так что РНС не может учесть долговременные зависимости. Эта проблема может возникнуть и в традиционной нейронной сети, но в большинстве случае в РНС она проявляется более рельефно, потому что в РНС больше слоев (временных шагов), через которые проходит обратное распространение. \n",
    "\n",
    "Взрывные градиенты обнаруживаются проще, поскольку когда градиент становится слишком большим он превращается в NaN и процесс обучения аварийно завершается. Рост градиента можно контролировать, например, обрезая их по достижении заданного орога (R. Pascanu, T. Mikolov, Y. Benguo \"On the Difficulty of Training Recurrent Neural Networks\").\n",
    "\n",
    "Существует несколько подходов к смягчению проблемы исчезающих градиентов, в частности, хорошая инициализация матрицы $W$, использование функции активации ReLU вместо tanh и предобучение слоев без учителя, но наиболее популярны архитекуры LSTM и GRU. Они специально проектировались для борьбы с исчезающим градиетом и более эффективно обучаются долговременным зависимостям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Долгая краткосрочная память (LSTM)\n",
    "\n",
    "LSTM - вариант РНС, способный обучаться долгосрочным зависимостям. LSTM сети впервые были предложены Хохрайтером и Шмидхубером, а затем улучшены многими другими исследователями. Они хорошо работают для широкого круга задач и являются самым популярным типом РНС. \n",
    "\n",
    "В простой РНС для реализации рекуррентности используется комбинация скрытого состояния на пр. шаге и текущих входных данных в слое с функцией активации. В LSTM-сети рекурреньность реализуется аналогично, но tanh-слоев не один, а четыре, и взаимодействуют они весьма специфичным образом. На рис. показаны преобразования, применяемые к скрытому состоянию на временном шаге $t$:\n",
    "\n",
    "![](./img/rnn-pic4.png)\n",
    "\n",
    "Выглядит сложно, рассмотрим схему по шагам.\n",
    "\n",
    "Горизонтальная линия сверху изображает состояние ячейки $c$, оно представляет внутреннюю память блока. На линии снизу показано скрытое состояние, а вентили $i,f,o,g$ - механизмы, посредством которых LSTM-сеть обходит проблему исчезающего градиента. В процессе обучения LSTM находит параметры этих вентелей.\n",
    "\n",
    "Чтобы лучше понять как эти вентили модулируют скрытое состояние LSTM-сети, рассмотрим формулы вычисления скрытого состояния $h_t$ в момент $t$ по состоянию $h_{t-1}$ на предыдущем шаге:\n",
    "$$i=\\sigma(W_i h_{t-1} + U_{i}x_{t})$$\n",
    "$$f = \\sigma(W_f h_{t-1} + U_{f}x_{t})$$\n",
    "$$ o = \\sigma(W_{o}h_{t-1}+U_{o}x_{t})$$\n",
    "$$g = tanh(W_{g}h_{t-1} + U_{g}x_{t})$$\n",
    "$$c_t = (c_{t-1} \\otimes f) \\oplus ( g \\otimes i)$$\n",
    "$$ h_t = tanh(c_t) \\otimes o $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь $i,f,o$ - входной вентиль, вентиль забывания и выходной вентиль. Все они вычисляются по одним и тем же формулам, но с разными матрицами параметров. Сигмоидная функция модулирует выход вентилей, приводя их к диапазону $[0;1]$, поэтому порождаемый выходной вектор можно умножить поэлементно на другой вектор, чтобы определить какая часть второго вектора может пройти через первый.\n",
    "\n",
    "Вентиль забывания определяет, какую часть пред. состояния $h_{t-1}$ желательно пропустить дальше. Входной вентиль определяет, какую часть вновь вычисленного состояния для текущего входа $x_t$ пропустить, а выходной вентиль - какую часть внутреннего состояния передать сл. слою. Внутреннее скрытое состояние $g$ вычисляется на основе текущего входа $x_t$ и предыдущего скрытого состояния $h_{t-1}$. Отметим, что выражение для $g$ совпадает с аналогичным выражением для ячейки простой РНС, но в данном случае мы модулируем выход, смешивая его с выходом входного вентиля $i$.\n",
    "\n",
    "Зная $i,f,g,o$ мы можем вычислить состояние ячейки $c_t$ в момент времени $t$ в терминах произведения $c_{t-1}$ на вентиль забывания и произведения $g$ на входной вентиль $i$. Это и есть способ комбинирования предыдущего содержимого памяти с новым входом. Если вентиль забывания установлен в 0, то старое запомненное состояние полностью игнорируется, а если установить в 0 ходной вентиль, то игнорируется новое вычисленное состояние.\n",
    "\n",
    "Наконец, скрытое состояние $h_t$ в момент $t$ вычисляется путем умножения памяти $c_t$ на значение выходного вентиля. \n",
    "\n",
    "Важно понимать, что LSTM можно всегда подставить вместо ячейки типа SimpleRNN и единственная разница состоит в том, что LSTM устойчива к проблеме исчезающего градиента. Заменив ячейку РНС на LSTM, мы можем не волноваться ни о каких побочных эффектах.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример LSTM - анализ эмоциональной окраски\n",
    "\n",
    "Keras предоставляет слой LSTM, которым мы воспользуемся чтобы построить и обучить сеть РНС типа многие-к-одному. Сеть будет пренимать предложение (последовательность слов) и выдавать индикатор эмоциональной окраски. Обучающий набор состоит примерно из 7000 коротких предложений, предлагавшихся на конкурсе Kaggle UMICH SI650 по классификации эмоциональной окраски.\n",
    "\n",
    "Начинаем с импорта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Займемся исследовательским анализом данных. Нужно знать сколько уникальных слов в корпусе текстов и сколько слов в каждом предложении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "\n",
    "\n",
    "\n",
    "# Read training data and generate vocabulary\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(os.path.join(DATA_DIR, \"umich-sentiment-train.txt\"), 'r', encoding='utf-8')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "    num_recs += 1\n",
    "ftrain.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "2328\n"
     ]
    }
   ],
   "source": [
    "## Get some information about our corpus\n",
    "print(maxlen)            # 42\n",
    "print(len(word_freqs))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зная кол-во уникальных слов len(word_freqs), задаем фиксированный размер словаря, все остальные слова считаем несловарными и заменяем их фиктивным словом UNK (unknown). На этапе предсказания это позволит нам обрабатывать ранее не встречавшиеся слова как несловарные.\n",
    "\n",
    "Зная число слов в предложении (maxlen), мы можем задать фиксированную длину предложения и более короткие предложения дополнять нулями, а длинные обрезать. Хотя РНС способна обрабатывать последовательности перемненой длины, достигается эо обычно дополнением и обрезанием, как описано выше, или группировкой входных данных в пакеты, содержащие последовательности одинаковой длины. Будем использовать первый подход.\n",
    "\n",
    "Исходя из вычисленных показателей, задаем VOCABULAAZY SIZE равным 2328 + 2: 2328 слов словар плюс фиктивные слова UNK и PAD (использьзуется для дополнения предложений до фикированного числа слов, в нашем случае MAX_SENTENCE_LENGTH = 42).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 2340\n",
    "MAX_SENTENCE_LENGTH = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понадобится пара таблиц соответствия. ВХодные данные для РНС - индексы слов, причем лова упорядочены по убыванию частоты встречаемости в обучающем наборе. Таблицы соответствия позволят находить индекс по слову и слово по индексу (включая фиктивные слова PAD и UNK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 is UNK, 0 is PAD\n",
    "# We take MAX_FEATURES-1 featurs to accound for PAD\n",
    "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in \n",
    "                enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы преобразуем входые предложения в последовательности индексов слов, дополняя их до MAX_SENTENCE_LENGTH слов. Поскольку в нашем случае результат - бинарная величина (положительная или отрицательная тональность), обрабатывать метки не нужно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert sentences to sequences\n",
    "X = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = open(os.path.join(DATA_DIR, \"umich-sentiment-train.txt\"), 'r', encoding='utf-8')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pad the sequences (left padded with zeros)\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем набор данных на обучающий и тестовый в пропорции 80:20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5668, 42) (1418, 42) (5668,) (1418,)\n"
     ]
    }
   ],
   "source": [
    "# Split input into training and test\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, \n",
    "                                                random_state=42)\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже представлена структура нашей РНС:\n",
    "![](./img/rnn-pic5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные данные - последовательность индексов слов. Длина последовательности равна MAX_SENTENCE_LENGTH. Первому измерению тензора присваивается значение None, показывающее что размер пакета (число записей загружаемых в сеть за один раз), в момент определения сети неизвестен, он будет задан на этапе выполнения с помощью параметра batch_size.\n",
    "\n",
    "Таким образомэ, в предположении, что размер пакета пока неизвестен, входной тензор имеет форму (None, MAX_SENTENCE_LENGTH, 1). Такие тензоры подаются на вход слоя погружения размра EMBEDDING_SIZE, веса которого инициализированы небольшими случайными значениями и подлежат обучению. Этот слой преобразует входной тензор к форме (None, MAX_SENTENCE_LENGTH, EMBEDDING_SIZE). Выход слоя погружения задается в LSTM с длиной поселдовательности MAX_SENTENCE_LENGTH и размером выходного слоя HIDDEN)LAYER_SIZE. На выходе LSTM получается тензор формы (None, HIDDEN_LAYER_SIZE, MAX_SENTENCE_LENGTH). По умолчанию LSTM выводит единственный тензор формы (None, HIDDEN_LAYER_SIZE) в качестве результирующей последовательности (return_sequence=False). Он подается на вход плотного слоя с размеровм выхода 1 и сигмоидной функции активации, который выводит 0 (отрицательная окраска), или 1 (положительная окраска).\n",
    "\n",
    "При компиляции модели указывается бинарная перекрестная энтропия как функция потерь, и Adam - универсаьный оптимизатор.\n",
    "\n",
    "Гиперпараметры EMBEDDING_SIZE, HIDDEN_LAYER_SIZE, BATCH_SIZE и NUM_EPOCHS выбраны по результатам экспериментов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/10\n",
      "5668/5668 [==============================] - 24s - loss: 0.2323 - acc: 0.8968 - val_loss: 0.0509 - val_acc: 0.9817\n",
      "Epoch 2/10\n",
      "5668/5668 [==============================] - 19s - loss: 0.0303 - acc: 0.9899 - val_loss: 0.0392 - val_acc: 0.98241s - loss: 0.\n",
      "Epoch 3/10\n",
      "5668/5668 [==============================] - 19s - loss: 0.0078 - acc: 0.9984 - val_loss: 0.0427 - val_acc: 0.9838.99\n",
      "Epoch 4/10\n",
      "5668/5668 [==============================] - 19s - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0437 - val_acc: 0.9901\n",
      "Epoch 5/10\n",
      "5668/5668 [==============================] - 18s - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0508 - val_acc: 0.9880\n",
      "Epoch 6/10\n",
      "5668/5668 [==============================] - 18s - loss: 0.0072 - acc: 0.9981 - val_loss: 0.0429 - val_acc: 0.9880\n",
      "Epoch 7/10\n",
      "5668/5668 [==============================] - 18s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0514 - val_acc: 0.9901\n",
      "Epoch 8/10\n",
      "5668/5668 [==============================] - 18s - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0525 - val_acc: 0.9908\n",
      "Epoch 9/10\n",
      "5668/5668 [==============================] - 18s - loss: 7.0802e-04 - acc: 0.9996 - val_loss: 0.0543 - val_acc: 0.9901\n",
      "Epoch 10/10\n",
      "5668/5668 [==============================] - 18s - loss: 6.1101e-04 - acc: 0.9998 - val_loss: 0.0540 - val_acc: 0.9908\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, \n",
    "                    input_length=MAX_SENTENCE_LENGTH))\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На результатах выполнения программы видно как убивает потеря и растет верность.\n",
    "\n",
    "Построим графики зависимости потери и верности в зависимости от времени:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncgteAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2772b8f4588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss and accuracy\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель оценивается на полном тестовом наборе. Выбираем также несколько сл. предложений из тестового набора и печатаем предсказание РНС, метку и само предложение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376/1418 [============================>.] - ETA: 0sTest score: 0.054, accuracy: 0.991\n",
      "1\t1\ti love kirsten / leah / kate escapades and mission impossible tom as well ...\n",
      "1\t1\tbrokeback mountain was beautiful ...\n",
      "1\t1\tda vinci code was an awesome movie ...\n",
      "1\t1\tbecause i would like to make friends who like the same things i like , and i really like harry potter , so i thought that joining a community like this would be a good start .\n",
      "1\t1\ti love harry potter..\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
    "\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,42)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
    "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вентильный рекуррентный блок (GRU)\n",
    "\n",
    "Вентильный рекуррентный блок (gated rucurrent unit, GRU) - это вариант LSTM, впервые предложенный К. Чо (Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation). Он также обладает устойчивостью к проблеме исчезающего градиента, но его внутренняя структура проще, а потому и обучается он быстрее, т.е. для обновления скрытого состояния нужно меньше вычислений. На сл. рисунке показаны вентили в ячейке GRU:\n",
    "![](./img/rnn-pic6.png)\n",
    "\n",
    "Вместо трех вентелей в ячейке LSTM - входного, забывания и выходного, в ячейке GRU всего два вентиля: обновления $z$ и сброса $r$. Вентиль обновления определяет, какую часть предыдущего запомненного значения сохранить, а вентиль сброса - как смешивать новых вход с предыдущей памятью. Не существует никакого постоянного состояния ячейки, отличного от скрытого состояния, как в LSTM. Механизм GRU описывается сл. формулами:\n",
    "$$z = \\sigma(W_zh_{t-1} + U_z x_t)$$\n",
    "$$r = \\sigma (W_r h_{t-1} + U_rx_t)$$\n",
    "$$c = tanh(W_c (h_{t-1}\\otimes r) + U_c x_t $$\n",
    "$$ h_t = (z \\otimes c ) \\oplus ((1-z)\\otimes h_{t-1})$$\n",
    "\n",
    "Согласно эмпирическим оценкам качество GRU и LSTM сранимо, и нельзя дать априорную рекомендацю, какую модель выбрать для конкретной задачи. GRU быстрее обучаются и требуют меньше данных для достижения генерализации, но в ситуациях, когда обучающих данных достаточно, большая выразительность LSTM может давать лучшие результаты.\n",
    "\n",
    "Как и LSTM, GRU можно подставить вместо ячейки типа SimpleRNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
